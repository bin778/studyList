{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e4e84cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self,train_data,valid_data,config):\n",
    "    lowest_loss=np.inf\n",
    "    best_model=None\n",
    "    \n",
    "    for epoch_index in range(config.n_epochs):\n",
    "        train_loss=self._train(train_data[0],train_data[1],config)\n",
    "        valid_loss=self._validate(train_data[0],train_data[1],config)\n",
    "        \n",
    "        # You muse use deep copy to take a snapshot of current best weights.\n",
    "        if valid_loss <= lowest_loss:\n",
    "            lowest_loss=valid_loss\n",
    "            best_model=deepcopy(self.model.state_dict())\n",
    "        \n",
    "        print(\"Epoch(%d/%d): train_loss=%.4e valid_loss=%.4e lowest_loss=%.4e\" % (\n",
    "            epoch_index+1,\n",
    "            config.n_epochs,\n",
    "            train_loss,\n",
    "            valid_loss,\n",
    "            lowest_loss,\n",
    "        ))\n",
    "    \n",
    "    # Restore to best model.\n",
    "    self.model.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c86b00d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train(self,x,y,config):\n",
    "    self.model.train()\n",
    "    \n",
    "    x,y=self._batchify(x,y,config.batch_size)\n",
    "    total_loss=0\n",
    "    \n",
    "    for i, (x_i,y_i) in enumerate(zip(x,y)):\n",
    "        y_hat_i=self.model(x_i)\n",
    "        loss=self.crit(y_hat_i,y_i.squeeze())\n",
    "        \n",
    "        # Initialize the gredients of the model.\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if config.verbose>=2:\n",
    "            print(\"Train Iteration(%d/%d): loss=%.4e\" % (i+1,len(x),float(loss_i)))\n",
    "        \n",
    "        # Don't forget to detach to prevent memory leak.\n",
    "        total_loss+=float(loss_i)\n",
    "    \n",
    "    return total_loss/len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a52ac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _batchify(self,x,y,batch_size,ramdom_split=True):\n",
    "    if random_split:\n",
    "        indices=torch.randperm(x.size(0),device=x.device)\n",
    "        x_=torch.index_select(x,dim=0,index=indices)\n",
    "        y_=torch.index_select(y,dim=0,index=indices)\n",
    "    \n",
    "    x_=x_.split(batch_size,dim=0)\n",
    "    y_=y_.split(batch_size,dim=0)\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b79de714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate(self,x,t,config):\n",
    "    # Turn evaluation mode on.\n",
    "    self.model.eval()\n",
    "    \n",
    "    # Turn on the no_grad mode to make more efficintly\n",
    "    with torch.no_grad():\n",
    "        x,y=self._batchify(x,y,config.batch_size,random_split=False)\n",
    "        total_loss=0\n",
    "        \n",
    "        for i, (x_i,y_i) in enumerate(zip(x,y)):\n",
    "            y_hat_i=self.model(x_i)\n",
    "            loss_i=self.crit(y_hat_i,y_i.squeeze())\n",
    "            \n",
    "            if config.verbose>=2:\n",
    "                print(\"Valid Iteration(%d/%d): loss=%.4e\" % (i+1,len(x),float(loss_i)))\n",
    "            \n",
    "            total_loss+=float(loss_i)\n",
    "            \n",
    "        return total_loss/len(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
